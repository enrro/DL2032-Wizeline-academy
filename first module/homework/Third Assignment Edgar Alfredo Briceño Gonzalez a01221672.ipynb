{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edgar Alfredo BriceÃ±o Gonzalez\n",
    "\n",
    "# A01221672\n",
    "\n",
    "---\n",
    "\n",
    "comments in > markdown notation and at the bottom of the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "During this session, you will participate in a supervised learning exercise about digit recognition. You will:\n",
    "- Build a neural network.\n",
    "- Train your neural network with a dataset that contains images that represent numbers. \n",
    "- Modify the architecture of the neural network to obtain better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST data\n",
    "\n",
    "\n",
    "The MNIST data is comprised of pictures that represent a number and includes the number label associated to each picture. The data set is split into three data parts:\n",
    "- training (mnist.train)\n",
    "- testing (mnist.test)\n",
    "- validation (mnist.validation)\n",
    "\n",
    "The validation split is important because it's essential in machine learning that there is separate data which is not given to the machine during the learning phase. After the initial machine learning session, you present the separate data to the machine and assess the performance of it. The test and validation sets may serve different evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "mnist = input_data.read_data_sets(\"./Data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "Common numerical computing libraries in Python use external code that is implemented in other languages to take advantage of their efficiency. However, switching back to Python for every operation causes overhead. This overhead is bad if you want to run computations on GPUs or in a distributed manner since there is a high cost to transfer data.\n",
    "\n",
    "TensorFlow does its heavy lifting outside of Python. Although it does not run expensive operations independently from Python, TensorFlow enables you to describe a graph of interacting operations that run entirely outside Python.\n",
    "\n",
    "#### TensorFlow Graph\n",
    "Create a TensorFlow graph that represents a neural network with no hidden layers and an output layer comprised of 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Write your code here\n",
    "########################\n",
    "\n",
    "#control variables\n",
    "image_size = 28*28\n",
    "number_digits = 10\n",
    "next_layer_neurons = 10\n",
    "\n",
    "#paceholders to build the graph\n",
    "x = tf.placeholder(tf.float32, [None, image_size])\n",
    "y_ = tf.placeholder(tf.float32, [None, number_digits])\n",
    "\n",
    "#initialize weights and bias to zero\n",
    "W = tf.Variable(tf.zeros([image_size, next_layer_neurons]))\n",
    "b = tf.Variable(tf.zeros([next_layer_neurons]))\n",
    "\n",
    "#matrix multiplication\n",
    "y = tf.add(tf.matmul(x, W), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Cost Funtion and Optimizer\n",
    "\n",
    "In order to train the model, define what it means to improve the results after each iteration. Use a cost function and try to minimize with respect to it. The cost function represents how far you are from our desired outcome. Minimizing the error leads you towards improving the model.\n",
    "\n",
    "A common cost function is called *cross-entropy*. Cross-entropy takes advantage of large errors and reduces the learning slow down that is caused because of traditional cost functions (i.e. quadratic cost function). In summary, it will take less to train a good model.\n",
    "\n",
    "#### TensorFlow\n",
    "1. Create a tensor to represent the cross-entropy function. \n",
    "1. Create a tensor to represent a Gradient Descent Optimizer that minimizes the cross-entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Write your code here\n",
    "########################\n",
    "#Softmax is one of many cost functions\n",
    "cross_entropy = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=y_, logits=y))\n",
    "\n",
    "#initialize the weights and bias to zeros\n",
    "learn_rate = 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(\n",
    "                learning_rate=learn_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a TensorFlow Session\n",
    "\n",
    "You have defined your model by creating a complete Tensorflow graph. Now, you need to launch it. Create an interactive session and initialize all the variables defined before.\n",
    "\n",
    "#### Interactive Session\n",
    "Create an Interactive Session and run the global variable initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Write your code here\n",
    "########################\n",
    "#A session allows to run the neural network.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#Initialize memory for the architecture variables\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "MNIST is a large dataset. To train using a batch learning method is too time intensive in-between epochs. Therefore, use small batches of random data. This method is called stochastic training.\n",
    "\n",
    "#### Stochastic Training\n",
    "1. In a `for` loop, take 100 random samples from MNIST and run the train step using the resulting batches. \n",
    "1. Repeat the process as many times as necessary. \n",
    "1. Present all the training datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Write your code here\n",
    "########################\n",
    "#create batches of information\n",
    "iterations = 50\n",
    "batch_size = 1000\n",
    "\n",
    "#feed for batches\n",
    "for _ in range(iterations):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    sess.run(train_step,\n",
    "        feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your model\n",
    "\n",
    "To understand your model's precision, you need to compare your results with the expected output. To calculate the precision, you need to sum the correct classifications over the size of the testing dataset.\n",
    "#### Calculating Precision\n",
    "1. Create a Tensor that compares the model's output with the expected output. \n",
    "1. Determine the fraction that are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8899\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Write your code here\n",
    "########################\n",
    "#Compare if the prediction matches reality\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1),\n",
    "                                tf.argmax(y_, 1))\n",
    "#Compute accuracy based on binary result\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,\n",
    "                                    tf.float32))\n",
    "#Print the value of accuracy (Notice the TEST dataset)\n",
    "print(sess.run(accuracy,feed_dict={x:mnist.test.images,\n",
    "                                    y_:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your own Neural Network\n",
    "**Goal:** Train a neural network (NN) with accuracy of ~95% on the testing set.\n",
    "#### Steps:\n",
    "* Create a NN with 2 hidden layers and 300 nodes in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control Variables\n",
    "image_size = 784 # MNIST data input (img shape: 28*28)\n",
    "number_digits = 10\n",
    "next_layer_neurons = 10\n",
    "n_hidden_1 = 300 # 1st layer number of neurons\n",
    "n_hidden_2 = 300 # 2nd layer number of neurons\n",
    "#tf graph input\n",
    "x = tf.placeholder(tf.float32, [None, image_size])\n",
    "y_ = tf.placeholder(tf.float32, [None, number_digits])\n",
    "# layer weight and bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.zeros([image_size, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.zeros([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.zeros([n_hidden_2, number_digits]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([number_digits]))\n",
    "}\n",
    "def neural_net(x):\n",
    "    #First Layer - Sigmoid matrix multiplication\n",
    "    y0 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    #Second Layer - Sigmoid matrix multiplication\n",
    "    y1 = tf.add(tf.matmul(y0, weights['h2']), biases['b2'])\n",
    "    #Output Layer - Matrix multiplication\n",
    "    y2 = tf.matmul(y1, weights['out']) + biases['out']\n",
    "    return y2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">the part above helped me refactoring the code since I used a lot of trial and error in the beginning and was not sure of the results I ended up redoing the neural network many times, here are the results of the refactor\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "learn_rate = 0.1\n",
    "logits = neural_net(x)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "#Softmax is one of many cost functions\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                     logits=logits, labels=y_))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#Gradient Descent is one of many cost-function optimizers\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learn_rate)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "#A session allows to run the neural network.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#Initialize memory for the architecture variables\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Above is where is defined the optimizer and training_step this methods are called whenever there is an adjustment in the precision if the neural network. As it is I changed the `GradientDescentOptimizer` optimizer for the `AdamOptimizer` since in the documentation it says its more acurate and a self adjusted method I needed to modify the `learn_rate` fewer times.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train model with 10 epochs.\n",
    "* Print the accuracy at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Step 1, Minibatch Loss= 21.6030, Training Accuracy= 0.101\n",
      "Step 100, Minibatch Loss= 1.8083, Training Accuracy= 0.748\n",
      "Step 200, Minibatch Loss= 1.0136, Training Accuracy= 0.793\n",
      "Step 300, Minibatch Loss= 0.4827, Training Accuracy= 0.856\n",
      "Step 400, Minibatch Loss= 0.4172, Training Accuracy= 0.893\n",
      "Step 500, Minibatch Loss= 0.3111, Training Accuracy= 0.906\n",
      "0.9032\n",
      "Testing Accuracy: 0.9032\n",
      "Epoch: 2\n",
      "Step 1, Minibatch Loss= 0.3256, Training Accuracy= 0.919\n",
      "Step 100, Minibatch Loss= 0.3779, Training Accuracy= 0.897\n",
      "Step 200, Minibatch Loss= 0.2734, Training Accuracy= 0.920\n",
      "Step 300, Minibatch Loss= 0.3551, Training Accuracy= 0.909\n",
      "Step 400, Minibatch Loss= 0.3750, Training Accuracy= 0.904\n",
      "Step 500, Minibatch Loss= 0.2087, Training Accuracy= 0.929\n",
      "0.9129\n",
      "Testing Accuracy: 0.9129\n",
      "Epoch: 3\n",
      "Step 1, Minibatch Loss= 0.2336, Training Accuracy= 0.932\n",
      "Step 100, Minibatch Loss= 0.2682, Training Accuracy= 0.926\n",
      "Step 200, Minibatch Loss= 0.2560, Training Accuracy= 0.929\n",
      "Step 300, Minibatch Loss= 0.2627, Training Accuracy= 0.921\n",
      "Step 400, Minibatch Loss= 0.2204, Training Accuracy= 0.931\n",
      "Step 500, Minibatch Loss= 0.2883, Training Accuracy= 0.920\n",
      "0.9178\n",
      "Testing Accuracy: 0.9178\n",
      "Epoch: 4\n",
      "Step 1, Minibatch Loss= 0.2063, Training Accuracy= 0.932\n",
      "Step 100, Minibatch Loss= 0.2386, Training Accuracy= 0.935\n",
      "Step 200, Minibatch Loss= 0.3190, Training Accuracy= 0.900\n",
      "Step 300, Minibatch Loss= 0.2834, Training Accuracy= 0.920\n",
      "Step 400, Minibatch Loss= 0.3103, Training Accuracy= 0.913\n",
      "Step 500, Minibatch Loss= 0.3028, Training Accuracy= 0.915\n",
      "0.9081\n",
      "Testing Accuracy: 0.9081\n",
      "Epoch: 5\n",
      "Step 1, Minibatch Loss= 0.2669, Training Accuracy= 0.926\n",
      "Step 100, Minibatch Loss= 0.2665, Training Accuracy= 0.930\n",
      "Step 200, Minibatch Loss= 0.2710, Training Accuracy= 0.916\n",
      "Step 300, Minibatch Loss= 0.2498, Training Accuracy= 0.932\n",
      "Step 400, Minibatch Loss= 0.2597, Training Accuracy= 0.923\n",
      "Step 500, Minibatch Loss= 0.3387, Training Accuracy= 0.914\n",
      "0.9015\n",
      "Testing Accuracy: 0.9015\n",
      "Epoch: 6\n",
      "Step 1, Minibatch Loss= 0.2911, Training Accuracy= 0.921\n",
      "Step 100, Minibatch Loss= 0.2875, Training Accuracy= 0.920\n",
      "Step 200, Minibatch Loss= 0.2768, Training Accuracy= 0.915\n",
      "Step 300, Minibatch Loss= 0.2889, Training Accuracy= 0.916\n",
      "Step 400, Minibatch Loss= 0.2683, Training Accuracy= 0.922\n",
      "Step 500, Minibatch Loss= 0.2936, Training Accuracy= 0.917\n",
      "0.9065\n",
      "Testing Accuracy: 0.9065\n",
      "Epoch: 7\n",
      "Step 1, Minibatch Loss= 0.2249, Training Accuracy= 0.926\n",
      "Step 100, Minibatch Loss= 0.2329, Training Accuracy= 0.926\n",
      "Step 200, Minibatch Loss= 0.2919, Training Accuracy= 0.919\n",
      "Step 300, Minibatch Loss= 0.2452, Training Accuracy= 0.930\n",
      "Step 400, Minibatch Loss= 0.2419, Training Accuracy= 0.936\n",
      "Step 500, Minibatch Loss= 0.2333, Training Accuracy= 0.934\n",
      "0.9171\n",
      "Testing Accuracy: 0.9171\n",
      "Epoch: 8\n",
      "Step 1, Minibatch Loss= 0.2806, Training Accuracy= 0.920\n",
      "Step 100, Minibatch Loss= 0.2776, Training Accuracy= 0.927\n",
      "Step 200, Minibatch Loss= 0.2648, Training Accuracy= 0.931\n",
      "Step 300, Minibatch Loss= 0.2752, Training Accuracy= 0.915\n",
      "Step 400, Minibatch Loss= 0.2449, Training Accuracy= 0.941\n",
      "Step 500, Minibatch Loss= 0.2060, Training Accuracy= 0.943\n",
      "0.9171\n",
      "Testing Accuracy: 0.9171\n",
      "Epoch: 9\n",
      "Step 1, Minibatch Loss= 0.2640, Training Accuracy= 0.920\n",
      "Step 100, Minibatch Loss= 0.2513, Training Accuracy= 0.919\n",
      "Step 200, Minibatch Loss= 0.2597, Training Accuracy= 0.927\n",
      "Step 300, Minibatch Loss= 0.2288, Training Accuracy= 0.940\n",
      "Step 400, Minibatch Loss= 0.1901, Training Accuracy= 0.938\n",
      "Step 500, Minibatch Loss= 0.2684, Training Accuracy= 0.919\n",
      "0.9111\n",
      "Testing Accuracy: 0.9111\n",
      "Epoch: 10\n",
      "Step 1, Minibatch Loss= 0.3074, Training Accuracy= 0.915\n",
      "Step 100, Minibatch Loss= 0.3246, Training Accuracy= 0.915\n",
      "Step 200, Minibatch Loss= 0.2457, Training Accuracy= 0.932\n",
      "Step 300, Minibatch Loss= 0.2327, Training Accuracy= 0.936\n",
      "Step 400, Minibatch Loss= 0.2574, Training Accuracy= 0.920\n",
      "Step 500, Minibatch Loss= 0.2641, Training Accuracy= 0.921\n",
      "0.9075\n",
      "Testing Accuracy: 0.9075\n",
      "Epoch: 11\n",
      "Step 1, Minibatch Loss= 0.2715, Training Accuracy= 0.914\n",
      "Step 100, Minibatch Loss= 0.2425, Training Accuracy= 0.931\n",
      "Step 200, Minibatch Loss= 0.2714, Training Accuracy= 0.928\n",
      "Step 300, Minibatch Loss= 0.2708, Training Accuracy= 0.921\n",
      "Step 400, Minibatch Loss= 0.2657, Training Accuracy= 0.918\n",
      "Step 500, Minibatch Loss= 0.2896, Training Accuracy= 0.913\n",
      "0.8992\n",
      "Testing Accuracy: 0.8992\n",
      "Epoch: 12\n",
      "Step 1, Minibatch Loss= 0.3165, Training Accuracy= 0.898\n",
      "Step 100, Minibatch Loss= 0.2641, Training Accuracy= 0.918\n",
      "Step 200, Minibatch Loss= 0.2542, Training Accuracy= 0.929\n",
      "Step 300, Minibatch Loss= 0.2888, Training Accuracy= 0.906\n",
      "Step 400, Minibatch Loss= 0.2486, Training Accuracy= 0.924\n",
      "Step 500, Minibatch Loss= 0.2696, Training Accuracy= 0.916\n",
      "0.9039\n",
      "Testing Accuracy: 0.9039\n",
      "Epoch: 13\n",
      "Step 1, Minibatch Loss= 0.2968, Training Accuracy= 0.908\n",
      "Step 100, Minibatch Loss= 0.3277, Training Accuracy= 0.907\n",
      "Step 200, Minibatch Loss= 0.2870, Training Accuracy= 0.906\n",
      "Step 300, Minibatch Loss= 4259.3564, Training Accuracy= 0.848\n",
      "Step 400, Minibatch Loss= 1643.5732, Training Accuracy= 0.847\n",
      "Step 500, Minibatch Loss= 1810.9489, Training Accuracy= 0.889\n",
      "0.8709\n",
      "Testing Accuracy: 0.8709\n",
      "Epoch: 14\n",
      "Step 1, Minibatch Loss= 2224.6465, Training Accuracy= 0.854\n",
      "Step 100, Minibatch Loss= 2463.7590, Training Accuracy= 0.780\n",
      "Step 200, Minibatch Loss= 6055.1680, Training Accuracy= 0.619\n",
      "Step 300, Minibatch Loss= 1758.3137, Training Accuracy= 0.854\n",
      "Step 400, Minibatch Loss= 386.5936, Training Accuracy= 0.902\n",
      "Step 500, Minibatch Loss= 338.0162, Training Accuracy= 0.902\n",
      "0.893\n",
      "Testing Accuracy: 0.893\n",
      "Epoch: 15\n",
      "Step 1, Minibatch Loss= 337.0663, Training Accuracy= 0.894\n",
      "Step 100, Minibatch Loss= 309.4935, Training Accuracy= 0.897\n",
      "Step 200, Minibatch Loss= 2412.2649, Training Accuracy= 0.795\n",
      "Step 300, Minibatch Loss= 211.3299, Training Accuracy= 0.919\n",
      "Step 400, Minibatch Loss= 324.2748, Training Accuracy= 0.905\n",
      "Step 500, Minibatch Loss= 239.6061, Training Accuracy= 0.913\n",
      "0.9078\n",
      "Testing Accuracy: 0.9078\n",
      "Epoch: 16\n",
      "Step 1, Minibatch Loss= 295.0033, Training Accuracy= 0.905\n",
      "Step 100, Minibatch Loss= 133.4864, Training Accuracy= 0.933\n",
      "Step 200, Minibatch Loss= 225.4852, Training Accuracy= 0.875\n",
      "Step 300, Minibatch Loss= 132.6965, Training Accuracy= 0.920\n",
      "Step 400, Minibatch Loss= 206.3081, Training Accuracy= 0.901\n",
      "Step 500, Minibatch Loss= 151.7544, Training Accuracy= 0.912\n",
      "0.9046\n",
      "Testing Accuracy: 0.9046\n",
      "Epoch: 17\n",
      "Step 1, Minibatch Loss= 140.8319, Training Accuracy= 0.907\n",
      "Step 100, Minibatch Loss= 78.1279, Training Accuracy= 0.932\n",
      "Step 200, Minibatch Loss= 218.4928, Training Accuracy= 0.913\n",
      "Step 300, Minibatch Loss= 446.9606, Training Accuracy= 0.698\n",
      "Step 400, Minibatch Loss= 78.4680, Training Accuracy= 0.909\n",
      "Step 500, Minibatch Loss= 246.0052, Training Accuracy= 0.889\n",
      "0.8882\n",
      "Testing Accuracy: 0.8882\n",
      "Epoch: 18\n",
      "Step 1, Minibatch Loss= 281.7013, Training Accuracy= 0.854\n",
      "Step 100, Minibatch Loss= 85.7086, Training Accuracy= 0.880\n",
      "Step 200, Minibatch Loss= 203.8179, Training Accuracy= 0.854\n",
      "Step 300, Minibatch Loss= 41.6657, Training Accuracy= 0.912\n",
      "Step 400, Minibatch Loss= 43.9847, Training Accuracy= 0.934\n",
      "Step 500, Minibatch Loss= 68.1359, Training Accuracy= 0.895\n",
      "0.8911\n",
      "Testing Accuracy: 0.8911\n",
      "Epoch: 19\n",
      "Step 1, Minibatch Loss= 42.5258, Training Accuracy= 0.920\n",
      "Step 100, Minibatch Loss= 58.8137, Training Accuracy= 0.926\n",
      "Step 200, Minibatch Loss= 31.9123, Training Accuracy= 0.923\n",
      "Step 300, Minibatch Loss= 26.0004, Training Accuracy= 0.929\n",
      "Step 400, Minibatch Loss= 84.2640, Training Accuracy= 0.901\n",
      "Step 500, Minibatch Loss= 30.5502, Training Accuracy= 0.904\n",
      "0.8859\n",
      "Testing Accuracy: 0.8859\n",
      "Epoch: 20\n",
      "Step 1, Minibatch Loss= 29.4536, Training Accuracy= 0.909\n",
      "Step 100, Minibatch Loss= 163.4743, Training Accuracy= 0.885\n",
      "Step 200, Minibatch Loss= 30.4056, Training Accuracy= 0.915\n",
      "Step 300, Minibatch Loss= 37.1120, Training Accuracy= 0.860\n",
      "Step 400, Minibatch Loss= 24.3567, Training Accuracy= 0.916\n",
      "Step 500, Minibatch Loss= 34.2365, Training Accuracy= 0.918\n",
      "0.9029\n",
      "Testing Accuracy: 0.9029\n"
     ]
    }
   ],
   "source": [
    "#Create epochs and batches of information\n",
    "epochs = 20\n",
    "num_steps = 500\n",
    "batch_size = 1000\n",
    "display_step = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #Feed the batches\n",
    "    print(\"Epoch: \" + str(epoch+1))\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization step (backprop)\n",
    "        sess.run(train_step, feed_dict={x: batch_x, y_: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cross_entropy, accuracy], feed_dict={x: batch_x,\n",
    "                                                                 y_: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    \n",
    "    #Print the value of accuracy (Notice the TEST dataset)\n",
    "    print(sess.run(accuracy,feed_dict={x:mnist.test.images,\n",
    "                                    y_:mnist.test.labels}))\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                      y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Print the final accuracy of the training set.\n",
    "* Print the final accuracy of the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8834\n"
     ]
    }
   ],
   "source": [
    "iterations = 500\n",
    "batch_size = 1000\n",
    "\n",
    "#feed for batches\n",
    "for _ in range(iterations):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    sess.run(train_step,\n",
    "        feed_dict={x: batch_x, y_: batch_y})\n",
    "#######################\n",
    "# Write your code here\n",
    "########################\n",
    "#Compare if the prediction matches reality\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1),\n",
    "                                tf.argmax(y_, 1))\n",
    "#Compute accuracy based on binary result\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,\n",
    "                                    tf.float32))\n",
    "#Print the value of accuracy (Notice the TEST dataset)\n",
    "print(sess.run(accuracy,feed_dict={x:mnist.test.images,\n",
    "                                    y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "During the exercise you:\n",
    "- Learned how to create a neural network and train it. \n",
    "- Tested the performance of your model, and found that the network architecture definition is important to obtain better results.\n",
    "- Learned the importance of the initialization step (random numbers instead of zeroes), and how it impacts performance. \n",
    "- Reviewed the activation functionsâ impact in performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and observations.\n",
    "this practice teached me how important is to have a proper learn_rate and to have a big enought sample of epochs to achieve an appropriate answer, at first I was stunned, I couldn't understand why my result was so far of the first one with no hidden layers, It was not until later after some tweaking numbers that It strucked my that it was very important to have the correct `learn_rate` and `num_steps` later I got to experiment with other tenser flow methods and it ended up with better results.\n",
    "\n",
    "By means of the `GradientDescentOptimizer` I have only achieved poor performance where the best optimization I have found is around 40 epocs with a `learn_rate` of 0.1. I have done other experiments alternating values in `num_steps`,` epochs`, `learn_rate` and` batch_size`, until now my experimentation has led me to understand that a size of steps like 50 that I had in the beginning is very little to achieve an acceptable `training_acuracy`.\n",
    "\n",
    "The following table depicts the results from diferent learning rates and their accuracy after 20 epochs.\n",
    "\n",
    "\n",
    "| learn_rate| accuracy|\n",
    "| --------- |:-------:|\n",
    "| 0.5       | .152    |\n",
    "| 0.2       | .122    |\n",
    "| 0.1       | .870     |\n",
    "\n",
    "the with Train a neural network (NN) with accuracy of ~95% was achieved with the `AdamOptimizer` training with a learing rate of .1 and 20 epochs. \n",
    "![proof](https://i.imgur.com/FVBYk2Y.png)\n",
    "\n",
    "## references.\n",
    "https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "\n",
    "https://stackoverflow.com/questions/40368697/where-does-next-batch-in-the-tensorflow-tutorial-batch-xs-batch-ys-mnist-trai\n",
    "\n",
    "https://stackoverflow.com/questions/40951602/what-does-the-question-mark-in-tensorflow-shape-mean\n",
    "\n",
    "https://towardsdatascience.com/under-the-hood-of-neural-network-forward-propagation-the-dreaded-matrix-multiplication-a5360b33426"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
